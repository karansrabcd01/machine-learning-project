{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Uploading the Dataset from the Kaggle\n",
        "The dataset is amazon-product-reviews and the link of the dataset is :-  https://www.kaggle.com/datasets/arhamrumi/amazon-product-reviews\n",
        "\n"
      ],
      "metadata": {
        "id": "lYCoB9QPtRwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()  # Upload the kaggle.json file\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json  # Secure the API key\n",
        "!pip install kaggle\n",
        "!kaggle datasets list\n",
        "!kaggle datasets download -d arhamrumi/amazon-product-reviews\n",
        "import zipfile\n",
        "\n",
        "# Unzip the dataset\n",
        "!unzip amazon-product-reviews.zip -d amazon_reviews\n"
      ],
      "metadata": {
        "id": "fkArCn0M_VjA",
        "outputId": "1dc13521-829b-419c-83cc-b464ce70d11d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-30800bcc-3ea0-42af-aa41-8fddcd971e3e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-30800bcc-3ea0-42af-aa41-8fddcd971e3e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "ref                                                        title                                                  size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "---------------------------------------------------------  -----------------------------------------------  ----------  --------------------------  -------------  ---------  ---------------  \n",
            "adilshamim8/student-depression-dataset                     Student Depression Dataset                           467020  2025-03-13 03:12:30.423000          15414        243  1.0              \n",
            "zahidmughal2343/amazon-sales-2025                          Amazon Sales 2025                                      3617  2025-04-03 22:08:13.607000           2613         40  1.0              \n",
            "atharvasoundankar/chocolate-sales                          Chocolate Sales Data üìäüç´                               14473  2025-03-19 03:51:40.270000          23296        369  1.0              \n",
            "atharvasoundankar/spotify-global-streaming-data-2024       üéß Spotify Global Streaming Data (2024)                14109  2025-04-07 06:26:31.707000           2601         45  1.0              \n",
            "atharvasoundankar/fashion-retail-sales                     üõçÔ∏è Fashion Retail Sales Dataset                       31656  2025-04-01 05:05:48.020000           1424         27  1.0              \n",
            "vinothkannaece/mobiles-and-laptop-sales-data               Mobiles & laptop Sales Data                         3242055  2025-03-24 05:03:52.657000           2179         30  1.0              \n",
            "adilshamim8/math-students                                  Math-Students Performance Data                         7367  2025-04-02 02:47:02.760000           1562         30  1.0              \n",
            "aradhanahirapara/product-retail-price-survey-2017-2025     Product Retail Prices per month from 2017-2025      2543973  2025-04-13 15:43:45.517000           1267         28  1.0              \n",
            "atharvasoundankar/impact-of-ai-on-digital-media-2020-2025  üåç Impact of AI on Digital Media (2020-2025)            5812  2025-04-03 09:12:25.070000           3194         58  1.0              \n",
            "ak0212/global-sugar-consumption-trends-19602023            Global Sugar Consumption Trends (1960‚Äì2023)         1658653  2025-03-25 04:13:38.503000           1572         27  1.0              \n",
            "ak0212/average-daily-screen-time-for-children              Average Daily Screen Time for Children                 1378  2025-03-24 03:52:51.137000           4327         59  1.0              \n",
            "zahidmughal2343/video-games-sale                           Video Games Sale                                     391554  2025-03-21 20:43:31.670000           2406         34  1.0              \n",
            "uom190346a/ai-generated-ghibli-style-image-trends-2025     AI Generated Ghibli Style Image Trends (2025)         30220  2025-04-02 04:15:31.277000            988         35  1.0              \n",
            "khushikyad001/water-pollution-and-disease                  Water Pollution & Disease                            148486  2025-04-03 15:46:19.363000           2169         29  1.0              \n",
            "atharvasoundankar/global-cybersecurity-threats-2015-2024   üåê Global Cybersecurity Threats (2015-2024)            48178  2025-03-16 04:23:13.343000           4924         81  1.0              \n",
            "tiagoadrianunes/imdb-top-5000-movies                       IMDb Top 5000 Movies                                 369999  2025-04-15 03:13:45.987000           1363         27  1.0              \n",
            "niharpatel03/h-and-m-product-dataset                       H&M Scrapped Product Dataset                        1764287  2025-04-09 20:18:30.813000            904         27  1.0              \n",
            "ak0212/uae-cancer-patient-dataset                          UAE Cancer Patient Dataset                           283039  2025-03-20 06:04:59.100000           1405         26  1.0              \n",
            "mahdimashayekhi/health-and-lifestyle-dataset               Health & Lifestyle Dataset                            25167  2025-04-01 13:28:38.930000           1891         31  0.7058824        \n",
            "dansbecker/melbourne-housing-snapshot                      Melbourne Housing Snapshot                           461423  2018-06-05 12:52:24.087000         176546       1599  0.7058824        \n",
            "Dataset URL: https://www.kaggle.com/datasets/arhamrumi/amazon-product-reviews\n",
            "License(s): CC0-1.0\n",
            "Archive:  amazon-product-reviews.zip\n",
            "  inflating: amazon_reviews/Reviews.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "-_ouXqcNABSu",
        "outputId": "9c3e3152-f7f0-4fad-807b-36f46d013eb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=f8a9f80692a99c322e85a178f8a384ecb15af722552b611e6e0f9111f7aeee0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers\n"
      ],
      "metadata": {
        "id": "YPrTQgWYA2qG",
        "outputId": "d21efeca-7d45-4bdd-d094-e20e6ad0d90d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.1\n",
            "    Uninstalling transformers-4.51.1:\n",
            "      Successfully uninstalled transformers-4.51.1\n",
            "Successfully installed transformers-4.51.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install warnings"
      ],
      "metadata": {
        "id": "swfb2f6GgIER",
        "outputId": "837a5d0c-d6f2-42fc-fbf8-77ca1d50a667",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement warnings (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for warnings\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the Libraries"
      ],
      "metadata": {
        "id": "YPZxMBQVuiNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import datasets\n",
        "import evaluate\n",
        "from datasets import Dataset\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Mxd6_Ar-pbAF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download** ALL necessary NLTK data with explicit download commands"
      ],
      "metadata": {
        "id": "ExyRyF0DuwsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Downloading NLTK data...\")\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "Z__mHdw5pmCe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f08317b3-ded1-424c-eaf8-e06ed6afb5bb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check** for GPU availability"
      ],
      "metadata": {
        "id": "r4N5Su80u0tN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "T3QfaqVcpqId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67a5c03b-ca56-4fe5-a595-037652bccaa1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset"
      ],
      "metadata": {
        "id": "RQpO92oNu-HR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading dataset...\")\n",
        "df = pd.read_csv(\"/content/amazon_reviews/Reviews.csv\", usecols=[\"Text\", \"Summary\"]).dropna()\n",
        "print(f\"Total dataset size: {len(df)} rows\")"
      ],
      "metadata": {
        "id": "nrIy92CRptcY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f5ae542-be6e-4dd9-99a2-c8567d175f6f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Total dataset size: 568427 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean** and preprocess text"
      ],
      "metadata": {
        "id": "ugutCNtsvAqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Clean text by removing HTML tags and extra whitespace\"\"\"\n",
        "    text = re.sub(r'<br\\s*/?>', ' ', text)  # Remove HTML line breaks\n",
        "    text = re.sub(r'\\s+', ' ', text)        # Replace multiple spaces with single space\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "fjmWwIDhpwtn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess the dataset"
      ],
      "metadata": {
        "id": "DgRys11VvG93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Preprocessing dataset...\")\n",
        "df['Text'] = df['Text'].apply(clean_text)\n",
        "df['Summary'] = df['Summary'].apply(clean_text)"
      ],
      "metadata": {
        "id": "TUevCTyXp04A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83d405ba-d7d2-48d7-9ab1-64e5a5d070dc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove** very short reviews and summaries (likely noise)"
      ],
      "metadata": {
        "id": "PVRPX5c2vqPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[(df['Text'].str.split().str.len() >= 5) &\n",
        "        (df['Summary'].str.split().str.len() >= 2)]"
      ],
      "metadata": {
        "id": "RchQaxROp7LG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Take only 10000 rows for training"
      ],
      "metadata": {
        "id": "1lFjZsCivwEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df = df.sample(n=10000, random_state=42)"
      ],
      "metadata": {
        "id": "xFY8giVjp-jM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split into train, validation, and test sets"
      ],
      "metadata": {
        "id": "tSbSscSZvyjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, temp_df = train_test_split(sampled_df, test_size=4000, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=2000, random_state=42)\n",
        "\n",
        "print(f\"Training set: {len(train_df)} rows\")\n",
        "print(f\"Validation set: {len(val_df)} rows\")\n",
        "print(f\"Test set: {len(test_df)} rows\")"
      ],
      "metadata": {
        "id": "OfK8kmy_qGgL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4fd85ab-b072-4b29-8665-a0db3b338bc2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: 6000 rows\n",
            "Validation set: 2000 rows\n",
            "Test set: 2000 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choossing the Bart-Large model which is a smaller, faster model that will be easier to fine-tune with limited data"
      ],
      "metadata": {
        "id": "iUkU_djTv7Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"facebook/bart-base\" # Smaller than bart-large models\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "ebs6ev-KqKG9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "9d4a30a1b16740b995ed489135dfb76a",
            "6b2e78a2c1c64b78851ccbb8a7f9627e",
            "5e6cb4d4a53240c491d06b32ce2937de",
            "e07313e530064cbdb3a640a1ce4215e8",
            "b6b3870e559b4e2d87af9654f5dcdd5b",
            "2dbd103f9fae436eb13f4327b9e2a772",
            "b336a626b492481e8555426ba9342363",
            "1c2a1152831245b8ab74304410a90109",
            "d30bdd736052460aa186e0f877bb633d",
            "f50ff6264a2048b98b483e4f7a5ab23f",
            "5b613e4d91cb4050b11b0d298eb48ed0",
            "986e98811edb4d2a89a01845bfc01ab9",
            "48af327df4654fd09087a9c20c22f3b1",
            "0b49bfc9b0f648b4a5a28de2effe37b7",
            "e74614feb90e4b9d9436d6aa507fe070",
            "82745e2a6ac64157955274380638c6a0",
            "767e1944a597497f99a1de0120ad34c0",
            "3d0fed7801b0458db93fe174b32999dc",
            "b31907c5d2854b2a9d91870f89ab050e",
            "9b95dc0cef154bfa89d8b890af53939d",
            "2a791613028246059ca6030543abcd6a",
            "823c07864d1645c6ad48f7d8010d7633",
            "cbc415edc9d54c849c91ea36c5e65ba9",
            "a56dac31d25443118bcb557d1882711c",
            "57cbd78292d646fb9970228b414e7523",
            "b1a87b245e2a47e9945d23ee37455340",
            "d97d035e602e47ce985a9994a871431c",
            "329aa083c70d455090f2ef92186c8143",
            "1db60c29a2bf461e85e2590772ef0293",
            "75b30df408dc4aef9d2791624f9a5027",
            "270445a8d6ca45578689660f62b1250c",
            "c3494e28f1a34307a2ffc27c0acaa241",
            "c3c111bacc8d4381832e15c4e0a80aa6",
            "fab73858380a416897887d3e33e6980c",
            "5317c73dba2b43249cb296cfe29e8fbc",
            "88f8ad383dc44db5b34c228d9cc9d83b",
            "3e8ae0c376174de7909e6e6e36ee2dc4",
            "d6a474c74ec94f38a2125779dc312658",
            "120b18f1a5a0475fa59617a129a288e3",
            "992486fb81e54da5b5f2620428b29a76",
            "5d9cd6edb0094fe689b0760d4500b57e",
            "69e5cd2dff74431eb36a58a64eaa8ddd",
            "7079150a7288495984971aa6e975cd27",
            "74cbb59f98ba429c96e969af172021f7",
            "93de5dcb08ab4cb58e8bbd68e6d42561",
            "5a0161573f7840c794b86cd364fe590d",
            "ba7cb6c0fa52471c96f040a49020369a",
            "190e42ade27541c4a4619900c7a91ac6",
            "fd8fe6e6729843d182b045a59e103d17",
            "2063b4c0f94648248b8b1dbad5865e3a",
            "ed739ae7aaeb419d87623af3e191b900",
            "85b875d1ac8549dab706d25c8929b261",
            "6001b4cb653440789cdb85fea272679d",
            "f9422e1cd2e248eeace539121f44343f",
            "f519453e2ea744808410b876d1e83a77"
          ]
        },
        "outputId": "452960c0-f7f0-4753-a25f-4cebff3e5e81"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d4a30a1b16740b995ed489135dfb76a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "986e98811edb4d2a89a01845bfc01ab9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cbc415edc9d54c849c91ea36c5e65ba9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fab73858380a416897887d3e33e6980c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93de5dcb08ab4cb58e8bbd68e6d42561"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Maximum** lengths for input and output (reduced to save memory)"
      ],
      "metadata": {
        "id": "jpD5yYuJwLow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_INPUT_LENGTH = 384\n",
        "MAX_TARGET_LENGTH = 48"
      ],
      "metadata": {
        "id": "zJ3RuII-qMm3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare** the dataset for training"
      ],
      "metadata": {
        "id": "x74VLyCPwQOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"Text\"]\n",
        "    targets = examples[\"Summary\"]\n",
        "\n",
        "    # Tokenize inputs\n",
        "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n",
        "\n",
        "    # Tokenize targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=MAX_TARGET_LENGTH, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "OJwjs9SPqZfr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert** dataframes to HuggingFace datasets"
      ],
      "metadata": {
        "id": "hEHRZKiGwWg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)"
      ],
      "metadata": {
        "id": "3p2_gRuQqdnm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apply** preprocessing"
      ],
      "metadata": {
        "id": "AnIwuX20wa6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokenizing datasets...\")\n",
        "train_tokenized = train_dataset.map(preprocess_function, batched=True)\n",
        "val_tokenized = val_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "AMVJyUvoqhY7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "f44ade20c3a44a57ae610f183bc9a1d3",
            "7fcf0cd6d1e64e80bdc354fc446a7014",
            "a7516fb6c4c242bd94f20d47a8772638",
            "4e46d3c9b52d4d40a8e6415ce0ef4cb8",
            "065b8ca9a2ff4520bab39703230fcd99",
            "d1e0246c89fa4da29d30a38d7a992f8b",
            "c8257a19876845f5a09714c902771e28",
            "2f09c249140540c9a61146d9514a168d",
            "e7c74271c28c44c3bf836485bbce1861",
            "049ba50b30ba47039ffc2b32339326ef",
            "e0f60a6bde0848d2b425be1f2d6fdab6",
            "f2a090faf64a4337a3370080c00d791d",
            "dcf70b165f9540249620513cafb6ca93",
            "9192af9a4676459584ace379c83001be",
            "9e372d33e2784d0883064d7965d132a4",
            "b67b4ec7d2d740cfa0e3f322bbf56e9b",
            "9de648bc0ee2436793f8a0646d67e684",
            "81ab2cfe53c14dfe956acc0b513f1fa3",
            "e5a4425784864e6bb5974b2fa0b034f9",
            "9a7f256057494779a05cd05b75308c0e",
            "095fc85d6ac7464381d5d62755c953fa",
            "711194f3374243dc94109e9224e4f6cc"
          ]
        },
        "outputId": "9006c0d6-7f28-4607-b9d5-b1e426b67caf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing datasets...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/6000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f44ade20c3a44a57ae610f183bc9a1d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2a090faf64a4337a3370080c00d791d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ],
      "metadata": {
        "id": "f0LlHe7SqoNI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metric for evaluation\n",
        "rouge = evaluate.load(\"rouge\")\n"
      ],
      "metadata": {
        "id": "_VNxdm-vqoxH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "eede6a8b056b4bc6b72633e100a5b94d",
            "af4720217cbe4230b221b4b79658fb26",
            "7c777b7c73984a208dc2fcbbba45e84f",
            "2d0a97ca9d0f4deeaf51c9edfe29051d",
            "52b578ccd47942cea080613e7177d470",
            "43a95e5de498450f801e0c3b1dde4b54",
            "ffe23b3966414df5b10d53d1e6e56a0a",
            "268fa6476a4141678fc85cc7cb2a9ceb",
            "56135cdd34974c75bc2f05ab6567fafd",
            "ac431961e6ed4053ada62781c10e6cdb",
            "2aa27389f9114e0b8a781c530ee41405"
          ]
        },
        "outputId": "d310ad9c-540d-44fe-8647-3255a31b5bb8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eede6a8b056b4bc6b72633e100a5b94d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom** function for sentence splitting"
      ],
      "metadata": {
        "id": "rGG_6LQ8wkMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom function for sentence splitting that doesn't rely on nltk's sent_tokenize\n",
        "def custom_sent_tokenize(text):\n",
        "    \"\"\"A simple sentence tokenizer that splits on common sentence terminators\"\"\"\n",
        "    # Split the text on common sentence terminators\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    # Remove empty sentences\n",
        "    return [sent for sent in sentences if sent.strip()]\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    # Get predictions and labels\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Decode predictions\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in labels as we can't decode them\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    # Decode references\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Use our custom tokenizer instead of nltk's\n",
        "    decoded_preds = [\"\\n\".join(custom_sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(custom_sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "    # Compute ROUGE scores\n",
        "    result = rouge.compute(\n",
        "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
        "    )\n",
        "\n",
        "    # Add mean generated length\n",
        "    prediction_lens = [len(pred.split()) for pred in decoded_preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ],
      "metadata": {
        "id": "vrLZ8HW3q0Yz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training** arguments and Trainer"
      ],
      "metadata": {
        "id": "LhDX6lg4wsFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./amazon-review-summarizer-small\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    num_train_epochs=4,\n",
        "    predict_with_generate=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    gradient_accumulation_steps=2,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ],
      "metadata": {
        "id": "j5vHdpMRq6Ja"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune the model"
      ],
      "metadata": {
        "id": "kgMAF6hiw0gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Fine-tuning the model...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "9p8sXtWJrC3C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "7c7ca265-8975-4ddf-dc78-98a59bcbea2c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning the model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3000/3000 10:30, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.341400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.866700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>2.605300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.179800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>2.020300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.872500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3000, training_loss=2.4809959920247397, metrics={'train_runtime': 631.9123, 'train_samples_per_second': 37.98, 'train_steps_per_second': 4.747, 'total_flos': 2668692624998400.0, 'train_loss': 2.4809959920247397, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the fine-tuned model"
      ],
      "metadata": {
        "id": "XwdSJc6Mw7NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"./amazon-review-summarizer-small-final\"\n",
        "trainer.save_model(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "print(f\"Model saved to {model_path}\")\n"
      ],
      "metadata": {
        "id": "CkOcsFqKrOFb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a03199a3-7e83-428f-bdb5-f36cdf483f34"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to ./amazon-review-summarizer-small-final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate on the test set - using a direct approach to avoid **OverflowError**"
      ],
      "metadata": {
        "id": "rgmI2Kkaw_Df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the test set - using a direct approach to avoid OverflowError\n",
        "print(\"Evaluating on test set...\")\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "test_tokenized = test_dataset.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "id": "4i9W8op4rQLU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "9d3f1c281f4e4da59b2eb599e1975d61",
            "952e087f4fa34008a30710fd2b49e48d",
            "f7d359ca46d74b929d0b36eca288049f",
            "8819b116e8af448f8da38764e1b15462",
            "2f49ff47fdee45599f6a8890360a5dcf",
            "63e3d28175ae4eddac1c20fc815b350a",
            "9ae2415ab6084158a1d712283bd4bd68",
            "2ad2877f44e84ebdade0d0a62a6d78a9",
            "c376d9215bde484eba62994cd2df0c36",
            "4aea5ff7984b48618cd4745e9e674af5",
            "8a2bde8acabc4e85bb4cbcd93dea2ce0"
          ]
        },
        "outputId": "22b85382-a3d4-411f-d123-344445845bd9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d3f1c281f4e4da59b2eb599e1975d61"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function for generating summaries"
      ],
      "metadata": {
        "id": "x1kHMXWQxGSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(text, max_length=MAX_TARGET_LENGTH):\n",
        "    # Clean the text\n",
        "    cleaned_text = clean_text(text)\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(cleaned_text, max_length=MAX_INPUT_LENGTH, truncation=True, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate summary\n",
        "    with torch.no_grad():\n",
        "        summary_ids = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            attention_mask=inputs.get(\"attention_mask\", None),\n",
        "            max_length=max_length,\n",
        "            min_length=10,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "gVWT7YsfrnWQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manually** evaluate on test set"
      ],
      "metadata": {
        "id": "hUHjIOvqxLtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating summaries for test set...\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "test_summaries = []\n",
        "rouge_scores = []\n",
        "\n",
        "for i, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "    # Generate summary\n",
        "    summary = generate_summary(row['Text'])\n",
        "\n",
        "    # Store results\n",
        "    test_summaries.append({\n",
        "        'Text': row['Text'][:200] + '...' if len(row['Text']) > 200 else row['Text'],\n",
        "        'Actual_Summary': row['Summary'],\n",
        "        'Generated_Summary': summary\n",
        "    })\n",
        "\n",
        "    # Calculate ROUGE score for this example using our custom tokenizer\n",
        "    pred_sentences = \"\\n\".join(custom_sent_tokenize(summary.strip()))\n",
        "    ref_sentences = \"\\n\".join(custom_sent_tokenize(row['Summary'].strip()))\n",
        "\n",
        "    score = rouge.compute(\n",
        "        predictions=[pred_sentences],\n",
        "        references=[ref_sentences],\n",
        "        use_stemmer=True\n",
        "    )\n",
        "    rouge_scores.append(score)"
      ],
      "metadata": {
        "id": "8VaIs2FwruvV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "c8835d10e05141adb1ec79957d8deeb6",
            "0c38dc40414a4754b46a7a0a5b375d0b",
            "803c2f7130e3495db326756106cfcf16",
            "d1495d0a9cc8452eaf95f056eee1c6ce",
            "022171271889418a904bef060d9fbaf5",
            "ccc876206a484c8d89f40a5221b6e182",
            "ae4d0111692d49d38023dd43ce0448ee",
            "364525e9e78f405b93b9a1038e93fd97",
            "7f774a8fd347473b891ab356bf78f94e",
            "a9eb3829ca2b4a949052aae06f119ce9",
            "985f506c205c4d709031c36ee8bd5185"
          ]
        },
        "outputId": "b0ea8ccd-fc57-490d-ebcf-2724e3b5deae"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating summaries for test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8835d10e05141adb1ec79957d8deeb6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create** results dataframe"
      ],
      "metadata": {
        "id": "746BvQQ2xQt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_results_df = pd.DataFrame(test_summaries)\n",
        "test_results_df.to_csv(\"amazon_review_test_summaries.csv\", index=False)"
      ],
      "metadata": {
        "id": "mt_sIIZNsFU0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate** average ROUGE scores and additional metrices and metrices"
      ],
      "metadata": {
        "id": "3kq36rU8xYJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average ROUGE scores\n",
        "avg_rouge = {key: np.mean([score[key] for score in rouge_scores]) for key in rouge_scores[0].keys()}\n",
        "print(\"\\nTest ROUGE Scores:\")\n",
        "for k, v in avg_rouge.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "# Calculate additional metrics\n",
        "def calculate_length_accuracy(row):\n",
        "    pred_len = len(row['Generated_Summary'].split())\n",
        "    ref_len = len(row['Actual_Summary'].split())\n",
        "    return 1 - min(abs(pred_len - ref_len) / max(ref_len, 1), 1)  # Bound between 0 and 1\n",
        "\n",
        "def calculate_token_overlap(row):\n",
        "    pred_tokens = set(row['Generated_Summary'].lower().split())\n",
        "    ref_tokens = set(row['Actual_Summary'].lower().split())\n",
        "    if not ref_tokens:\n",
        "        return 0\n",
        "    return len(pred_tokens.intersection(ref_tokens)) / len(ref_tokens)\n",
        "\n",
        "# Calculate metrics\n",
        "test_results_df['Length_Accuracy'] = test_results_df.apply(calculate_length_accuracy, axis=1)\n",
        "test_results_df['Token_Overlap'] = test_results_df.apply(calculate_token_overlap, axis=1)\n",
        "\n",
        "# Print average metrics\n",
        "avg_length_accuracy = test_results_df['Length_Accuracy'].mean()\n",
        "avg_token_overlap = test_results_df['Token_Overlap'].mean()\n",
        "print(f\"\\nAverage Length Accuracy: {avg_length_accuracy:.4f}\")\n",
        "print(f\"Average Token Overlap (simple accuracy): {avg_token_overlap:.4f}\")"
      ],
      "metadata": {
        "id": "UE6BUJSdsObT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78194839-a382-4bde-c32b-e2c6ab21801e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test ROUGE Scores:\n",
            "rouge1: 0.1673\n",
            "rouge2: 0.0555\n",
            "rougeL: 0.1590\n",
            "rougeLsum: 0.1599\n",
            "\n",
            "Average Length Accuracy: 0.3489\n",
            "Average Token Overlap (simple accuracy): 0.1754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample review function for demonstration"
      ],
      "metadata": {
        "id": "OrlehOoNxiiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_review(review_text):\n",
        "    \"\"\"\n",
        "    Function to summarize a product review\n",
        "\n",
        "    Parameters:\n",
        "    review_text (str): The product review text to summarize\n",
        "\n",
        "    Returns:\n",
        "    str: Concise summary of the review\n",
        "    \"\"\"\n",
        "    return generate_summary(review_text)"
      ],
      "metadata": {
        "id": "Ya9cJH1FsuE7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show examples from the test set\n",
        "print(\"\\nExample summaries:\")\n",
        "for i in range(min(5, len(test_results_df))):\n",
        "    print(f\"\\nReview {i+1}: {test_results_df.iloc[i]['Text']}\")\n",
        "    print(f\"Actual summary: {test_results_df.iloc[i]['Actual_Summary']}\")\n",
        "    print(f\"Generated summary: {test_results_df.iloc[i]['Generated_Summary']}\")"
      ],
      "metadata": {
        "id": "Raqt3u-3sxe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aaab208-cfa1-415b-80f0-25281ed7ca84"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example summaries:\n",
            "\n",
            "Review 1: The shipment arrived on time.The Keurig bundle is a perfect gif for coffee lovers.It contains aside assortment of flavors and blends\n",
            "Actual summary: Keurig Coffee\n",
            "Generated summary: Great Gift for Coffee Lovers and Coffee Drinkers\n",
            "\n",
            "Review 2: I was pleased with the price of these breakfast \"cookies\" since I generally try to find a larger granola-bar-type item to each each morning. The cookie is a good size, and it kept me full until lunch....\n",
            "Actual summary: Breakfast Cookie - wish it was organic\n",
            "Generated summary: Good, but not as good as I would like\n",
            "\n",
            "Review 3: I have tried a lot of different arthritis supplements for my small dogs, but this one is the best product I have found for the price. My dogs love it and it is wonderful to see them running around whe...\n",
            "Actual summary: Best Product for the Price!\n",
            "Generated summary: Best arthritis supplement I have found for the price\n",
            "\n",
            "Review 4: The tea is very good and tasty. As soon as you open the bag, you can smell a strong smell of all the spices like cardamom and cinnamon, which is very good. By the smell you can tell that the tea is a ...\n",
            "Actual summary: Amazing Taste\n",
            "Generated summary: Delicious and Tasty Tea! :)\n",
            "\n",
            "Review 5: Tast is more robust than I thought tasts very strong or burnt. Mabe my way of making it but just not for me.\n",
            "Actual summary: hill of beans\n",
            "Generated summary: Not as strong as I thought it would be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example of generating a summary for a new review"
      ],
      "metadata": {
        "id": "xPsmxmydxnxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of generating a summary for a new review\n",
        "print(\"\\nExample of generating a summary for a new review:\")\n",
        "example_review = \"\"\"\n",
        "I recently purchased this wireless speaker, and I‚Äôm impressed with the sound quality! The bass is deep, and the treble is clear. It pairs easily with my phone, and the Bluetooth range is excellent. The battery life is long-lasting, and it charges quickly. It‚Äôs also lightweight and portable, making it perfect for taking on the go. I love using it for outdoor gatherings!\n",
        "\"\"\"\n",
        "\n",
        "summary = summarize_review(example_review)\n",
        "print(f\"Input review: {example_review.strip()}\")\n",
        "print(f\"Generated summary: {summary}\")\n",
        "\n",
        "print(\"\\nProcessing complete!\")"
      ],
      "metadata": {
        "id": "SvVUD44N-pR-",
        "outputId": "0c2fa8e6-29a9-4261-c3ed-f0426e9a6d21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example of generating a summary for a new review:\n",
            "Input review: I recently purchased this wireless speaker, and I‚Äôm impressed with the sound quality! The bass is deep, and the treble is clear. It pairs easily with my phone, and the Bluetooth range is excellent. The battery life is long-lasting, and it charges quickly. It‚Äôs also lightweight and portable, making it perfect for taking on the go. I love using it for outdoor gatherings!\n",
            "Generated summary: Great sound, portable and great for outdoor gatherings\n",
            "\n",
            "Processing complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n3M-h9s77hjN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}